# snowflake-learning
Personal training on snowflake/dbt using weather data

## ğŸ¯ Project Overview
This project demonstrates a modern data pipeline using:
- **Weather API data** (Open-Meteo)
- **Snowflake** (cloud data warehouse)
- **dbt** (data transformation)
- **Bronze/Silver/Gold architecture** (medallion architecture)

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ models/              # dbt transformation models
â”‚   â”œâ”€â”€ bronze/         # Raw data parsing (bronze_weather.sql)
â”‚   â”œâ”€â”€ silver/         # Cleaned & enriched data (silver_weather.sql, fact_weather_enriched.sql)
â”‚   â”œâ”€â”€ gold/           # Business metrics & aggregations (avg_temp_by_country.sql, min_temp_by_city.py)
â”‚   â””â”€â”€ src/            # Source definitions (sources.yml)
â”œâ”€â”€ seeds/              # Reference data (dim_city.csv, dim_country.csv)
â”œâ”€â”€ snapshots/          # Historical dimension tracking (SCD Type 2)
â”œâ”€â”€ tests/              # Data quality tests
â”œâ”€â”€ analyses/           # Ad-hoc analytical queries
â”œâ”€â”€ macros/             # Reusable dbt macros (generate_schema_name.sql, round_coordinates.sql)
â”œâ”€â”€ setup/              # Database & Snowflake initialization scripts
â”œâ”€â”€ raw-data/           # Local JSON files from API fetches
â””â”€â”€ main.py             # Python script to fetch weather data from Open-Meteo API
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Set Snowflake credentials
$env:SNOWFLAKE_ACCOUNT="your_account"
$env:SNOWFLAKE_USER="your_user"
$env:SNOWFLAKE_PASSWORD="your_password"
```

### 2. Run Setup Scripts in Snowflake UI
```sql
-- In Snowflake, run these in order:
setup/init_db.sql              -- Create database (WEATHER_DB) & schemas (RAW, BRONZE, SILVER, GOLD)
setup/init_tables.sql          -- Create raw tables
setup/load_raw_data.sql        -- Load data from stage (@WEATHER_DB.RAW.UPLOADS)
setup/external_access.sql      -- (Optional) Setup external access
setup/deploy_dbt_to_snowsight.sql  -- (Optional) Deploy dbt project to Snowsight
```

### 3. Run dbt Pipeline
```bash
dbt seed             # Load dimension tables
dbt build            # Build all models
dbt test             # Run data quality tests
dbt snapshot         # Track dimension changes
```

## ğŸ“Š New Features Added

### âœ… Tests (Data Quality)
- **assert_temperature_realistic** - Ensures temp values are within -100Â°C to +60Â°C
- **assert_no_duplicate_hourly_readings** - Checks for duplicate hourly readings per city
- **Column-level tests** (in schema.yml) - Not null, relationships to dimensions

### ğŸ“¸ Snapshots (Historical Tracking)
- **dim_city_snapshot** - Tracks changes to city dimensions over time
- Uses SCD Type 2 strategy

### ğŸ“ˆ Analyses (Ad-hoc Queries)
- **Temperature anomalies** - Detect extreme temp changes
- **Country comparison** - Compare weather patterns by country
- **Hourly patterns** - Analyze daily temperature cycles


## ğŸ—ï¸ Architecture

### Data Flow
```
Open-Meteo API (main.py)
    â†“
Snowflake Stage (WEATHER_DB.RAW.UPLOADS)
    â†“
RAW Layer (WEATHER_DB.RAW.WEATHER)
    â†“
BRONZE Layer (Parse JSON)
    â†“
SILVER Layer (Clean & Flatten)
    â†“
GOLD Layer (Business Metrics)
```

### Schemas
- **RAW** - Raw JSON data from API
- **BRONZE** - Parsed JSON
- **SILVER** - Cleaned/enriched data + dimension tables
- **GOLD** - Business-ready aggregations
- **SNAPSHOTS** - Historical dimension tracking

## ğŸ”§ Technologies
- **dbt** - Data transformation framework
- **Snowflake** - Cloud data warehouse
- **Python** - API data fetching
- **SQL** - Data transformations

## ğŸ“„ Useful Links
- [Snowflake signup](https://signup.snowflake.com/)
- [dbt documentation](https://docs.getdbt.com/docs/introduction)
- [Snowflake SQL documentation](https://docs.snowflake.com/en/reference)
- [Snowpark DataFrame documentation](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/dataframe)
